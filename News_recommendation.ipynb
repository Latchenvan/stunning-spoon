{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Recommendation System\n",
    "\n",
    "This Jupyter notebook demonstrates the creation of a news recommendation system. The notebook is divided into several parts:\n",
    "1. Fetching and preprocessing news data.\n",
    "2. Storing data in a MySQL database.\n",
    "3. Simulating user interactions.\n",
    "4. Building and evaluating a collaborative filtering model using SVD.\n",
    "5. Creating a content-based recommendation system using TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fetching and Preprocessing News Data\n",
    "\n",
    "In this part, fetch the latest news headlines using the NewsAPI and preprocess the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock market news today: Nasdaq sinks, Nvidia drops 5% ahead of next round of Big Tech earnings - Yahoo Finance None\n",
      "Olympic gymnastics live updates: Simone Biles, USA women win gold medal in team final - USA TODAY None\n",
      "Israel targets Hezbollah commander in Beirut strike after deadly Golan Heights attack - CNN None\n",
      "Kamala Harris to campaign with VP pick in battleground states next week - CBS News None\n",
      "2024 Booker longlist includes ‘James’ and ‘Wandering Stars’ - The Washington Post The 13 finalists include Percival Everett’s retelling of “Adventures of Huckleberry Finn” and Tommy Orange’s follow-up to “There There.”\n",
      "'Stop interrupting me': Cruz gets in heated exchange with Secret Service official - CNN None\n",
      "Gunmen on jet skis kill 12-year-old boy on Cancun beach while firing at rival drug dealer: Mexican officials - Fox News None\n",
      "DUIs and integrity concerns: What we know about the deputy who killed Sonya Massey - USA TODAY None\n",
      "Probe of soldiers over alleged sexual abuse fuels tension between Israeli military and hard-liners - The Associated Press None\n",
      "Bob Dylan Experts Embrace Timothée Chalamet and Praise His Singing Voice After ‘A Complete Unknown’ Trailer: ‘Even the Most Cynical Fans Will Be Excited Now’ - Variety None\n",
      "California EDM festival is hit with deadly fungus outbreak that has hospitalized three and left one with 'a ho - Daily Mail None\n",
      "FBI official: Investigators found social media account believed to belong to Trump shooter - CNN FBI Deputy Director Paul Abbate said in a joint Senate hearing that the bureau is working to verify a social media account that appeared to reflect “antisemitic” and “anti-immigration” themes that is believed to be linked to the shooter who attempted to assas…\n",
      "Diminished Big Mac appetite is a sign of deflationary trend, says Yardeni - MarketWatch None\n",
      "Spirit Airlines is trying to go upmarket with snacks, Wi-Fi and checked bags included - CNBC None\n",
      "U.S. Gymnast Stephen Nedoroscik Goes Viral After Helping Team Secure Bronze—Here’s When He’ll Compete Next - Forbes None\n",
      "Google’s antitrust defense could benefit from the threat of SearchGPT - Yahoo Finance None\n",
      "Trump Again Says That Christians ‘Won’t Have to Vote Anymore’ if They Vote for Him - The New York Times None\n",
      "Tesla recalls 1.85 million US vehicles over unlatched hood issue - Reuters None\n",
      "Ozempic And Wegovy Could Help Smokers Quit, Study Suggests - Forbes None\n",
      "Gemini Daily Horoscope Today, July 31, 2024 predicts productive results - Hindustan Times None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_key = 'd6ba6f6958ea40deafd02344947bbe2b'\n",
    "url = f'https://newsapi.org/v2/top-headlines?country=us&apiKey={api_key}'\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "articles = data['articles']\n",
    "for article in articles:\n",
    "    print(article['title'], article['description'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cleaning and Preparing Data for TF-IDF Vectorization\n",
    "\n",
    "Clean the titles of the articles by removing any non-alphanumeric characters to prepare them for TF-IDF vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(articles)\n",
    "data['cleaned_title'] = data['title'].str.replace('[^a-zA-Z0-9\\s]', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(data['cleaned_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Setting Up MySQL Database and Tables\n",
    "\n",
    "Set up a MySQL database and create tables to store the news articles and user interactions.\n",
    "\n",
    "## Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245540.03s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in ./venv/lib/python3.11/site-packages (9.0.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: sqlalchemy in ./venv/lib/python3.11/site-packages (2.0.31)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in ./venv/lib/python3.11/site-packages (from sqlalchemy) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python pandas sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to MySQL\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='november281996',\n",
    "    database='news_recommendation'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create news_articles table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS news_articles (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    author VARCHAR(255),\n",
    "    title VARCHAR(255) NOT NULL,\n",
    "    description TEXT,\n",
    "    url VARCHAR(255) NOT NULL,\n",
    "    urlToImage VARCHAR(255),\n",
    "    publishedAt DATETIME,\n",
    "    content TEXT,\n",
    "    source_id VARCHAR(50),\n",
    "    source_name VARCHAR(100)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create user_interactions table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_interactions (\n",
    "    user_id INT,\n",
    "    article_id INT,\n",
    "    rating INT,\n",
    "    PRIMARY KEY (user_id, article_id)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Inserting Data into MySQL\n",
    "\n",
    "Insert the fetched news articles into the MySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame(articles)\n",
    "\n",
    "# Flatten the 'source' column\n",
    "data['source_id'] = data['source'].apply(lambda x: x.get('id') if x else None)\n",
    "data['source_name'] = data['source'].apply(lambda x: x.get('name') if x else None)\n",
    "data = data.drop(columns=['source'])\n",
    "\n",
    "# Convert 'publishedAt' to MySQL-compatible datetime format\n",
    "data['publishedAt'] = pd.to_datetime(data['publishedAt']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Truncate the URL if it exceeds the length limit (e.g., 255 characters)\n",
    "max_length = 255\n",
    "data['url'] = data['url'].apply(lambda x: x[:max_length] if x and len(x) > max_length else x)\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine('mysql+mysqlconnector://root:november281996@localhost/news_recommendation')\n",
    "\n",
    "# Insert data into news_articles table\n",
    "try:\n",
    "    data.to_sql('news_articles', con=engine, if_exists='append', index=False)\n",
    "    print(\"Data inserted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Simulating User Interactions and Inserting into MySQL\n",
    "\n",
    "Simulate user interactions with the news articles and store the interactions in the MySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User interactions successfully simulated and stored in MySQL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/dmn0rdm566x7mwf7yj4n74440000gp/T/ipykernel_69396/3475927824.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  articles = pd.read_sql(\"SELECT * FROM news_articles\", conn)\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, exc, text\n",
    "import numpy as np\n",
    "\n",
    "# Connect to MySQL and fetch news articles\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='november281996',\n",
    "    database='news_recommendation'\n",
    ")\n",
    "\n",
    "# Fetch articles into a DataFrame\n",
    "articles = pd.read_sql(\"SELECT * FROM news_articles\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Assuming you have a DataFrame 'articles' from your news articles collection\n",
    "article_ids = articles['id'].tolist()\n",
    "user_ids = range(1, 101)  # Simulate 100 users\n",
    "\n",
    "# Simulate interactions\n",
    "np.random.seed(42)\n",
    "interactions = []\n",
    "for user_id in user_ids:\n",
    "    # Each user interacts with 10-20 articles\n",
    "    interacted_articles = np.random.choice(article_ids, size=np.random.randint(10, 20), replace=False)\n",
    "    for article_id in interacted_articles:\n",
    "        rating = np.random.randint(1, 6)  # Simulate a rating between 1 and 5\n",
    "        interactions.append((user_id, article_id, rating))  # Append as a tuple\n",
    "\n",
    "# Create a DataFrame\n",
    "user_interactions = pd.DataFrame(interactions, columns=['user_id', 'article_id', 'rating'])\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine('mysql+mysqlconnector://root:november281996@localhost/news_recommendation')\n",
    "\n",
    "# Store interactions in MySQL with duplicate handling\n",
    "conn = engine.connect()\n",
    "\n",
    "# Prepare the data for insertion\n",
    "data_to_insert = user_interactions.to_dict(orient='records')\n",
    "\n",
    "# Create the query with placeholders\n",
    "query = text(\"\"\"\n",
    "    INSERT INTO user_interactions (user_id, article_id, rating)\n",
    "    VALUES (:user_id, :article_id, :rating)\n",
    "    ON DUPLICATE KEY UPDATE rating = VALUES(rating)\n",
    "\"\"\")\n",
    "\n",
    "# Use execute many for bulk insert\n",
    "try:\n",
    "    conn.execute(query, data_to_insert)\n",
    "except exc.SQLAlchemyError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "conn.close()\n",
    "print(\"User interactions successfully simulated and stored in MySQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Building and Evaluating a Recommendation Model with SVD\n",
    "\n",
    "Build a collaborative filtering recommendation model using Singular Value Decomposition (SVD) and evaluate its performance.\n",
    "\n",
    "## Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245546.05s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in ./venv/lib/python3.11/site-packages (1.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.11/site-packages (from scikit-surprise) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./venv/lib/python3.11/site-packages (from scikit-surprise) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.11/site-packages (from scikit-surprise) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5326\n",
      "RMSE: 1.5325882836327962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/dmn0rdm566x7mwf7yj4n74440000gp/T/ipykernel_69396/1084876990.py:16: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  user_interactions = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "\n",
    "# Connect to MySQL and fetch user interaction data\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='november281996',\n",
    "    database='news_recommendation'\n",
    ")\n",
    "query = \"SELECT * FROM user_interactions\"\n",
    "user_interactions = pd.read_sql(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Load data into Surprise dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(user_interactions[['user_id', 'article_id', 'rating']], reader)\n",
    "\n",
    "# Train and test split\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Train SVD algorithm\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = algo.test(testset)\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Content-Based Recommendation System Using TF-IDF\n",
    "\n",
    "Create a content-based recommendation system using TF-IDF vectorization and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the title: 21\n",
      "Similarity scores before sorting: [(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.05976902006767167), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.11412742304479827), (8, 0.0), (9, 0.0)]\n",
      "Filtered similarity scores: [(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.05976902006767167), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.11412742304479827), (8, 0.0), (9, 0.0)]\n",
      "Similarity scores after sorting: [(21, 0.9999999999999999), (70, 0.20842463708780345), (63, 0.179309214511483), (45, 0.15847010266977618), (7, 0.11412742304479827), (50, 0.1105487529575391), (48, 0.1014362279006179), (81, 0.06279512441457209), (11, 0.06110793175291705), (54, 0.06109826516994615)]\n",
      "Top 10 similarity scores: [(70, 0.20842463708780345), (63, 0.179309214511483), (45, 0.15847010266977618), (7, 0.11412742304479827), (50, 0.1105487529575391), (48, 0.1014362279006179), (81, 0.06279512441457209), (11, 0.06110793175291705), (54, 0.06109826516994615), (29, 0.06068136708669128)]\n",
      "Article indices: [70, 63, 45, 7, 50, 48, 81, 11, 54, 29]\n",
      "174    White Dudes for Kamala Harris: Giant Zoom call...\n",
      "167    Microsoft apologises after thousands report ne...\n",
      "149    Southport knife attack: Two children dead and ...\n",
      "8      D.C. cleans up vandalism and graffiti in wake ...\n",
      "154    Israeli officials say they want to avoid all-o...\n",
      "152    McDonald’s sales fall worldwide for first time...\n",
      "185    Kamala Harris to campaign with VP pick in batt...\n",
      "12     NASA's Perseverance Mars rover finds possible ...\n",
      "158    Roy Cooper Is Said to Withdraw From Harris’s V...\n",
      "130    SpaceX finds cause of Falcon 9 failure, eyes r...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine('mysql+mysqlconnector://root:november281996@localhost/news_recommendation')\n",
    "\n",
    "# Connect to MySQL and fetch news articles\n",
    "articles = pd.read_sql(\"SELECT * FROM news_articles\", engine)\n",
    "\n",
    "# Ensure titles are unique by removing duplicates\n",
    "articles = articles.drop_duplicates(subset='title')\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(articles['title'].fillna(''))\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "indices = pd.Series(articles.index, index=articles['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    if title not in indices:\n",
    "        return f\"Title '{title}' not found in the articles.\"\n",
    "    \n",
    "    idx = indices[title]\n",
    "    print(f\"Index of the title: {idx}\")\n",
    "\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    print(f\"Similarity scores before sorting: {sim_scores[:10]}\")  # Print first 10 scores for debugging\n",
    "\n",
    "    # Ensure that similarity scores are tuples of index and score\n",
    "    sim_scores = [(i, score) for i, score in sim_scores if isinstance(score, (int, float))]\n",
    "    print(f\"Filtered similarity scores: {sim_scores[:10]}\")  # Print first 10 scores for debugging\n",
    "\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    print(f\"Similarity scores after sorting: {sim_scores[:10]}\")  # Print first 10 scores for debugging\n",
    "\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    print(f\"Top 10 similarity scores: {sim_scores}\")  # Print top 10 scores for debugging\n",
    "\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "    print(f\"Article indices: {article_indices}\")  # Print article indices for debugging\n",
    "\n",
    "    return articles['title'].iloc[article_indices]\n",
    "\n",
    "# Test the function with a specific title\n",
    "test_title = 'Russian and Chinese bombers intercepted off of Alaska - ABC News'\n",
    "recommendations = get_recommendations(test_title)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Improved Content-Based Recommendation System with TF-IDF and Cosine Similarity\n",
    "\n",
    " Improve the content-based recommendation system by adding bigrams to the TF-IDF vectorization and ensuring robustness in the similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White Dudes for Kamala Harris: Giant Zoom calls power fundraising - BBC.com\n",
      "Microsoft apologises after thousands report new outage - BBC.com\n",
      "Southport knife attack: Two children dead and nine injured at dance workshop - BBC.com\n",
      "D.C. cleans up vandalism and graffiti in wake of anti-Netanyahu protests - The Washington Post\n",
      "Israeli officials say they want to avoid all-out war in Lebanon retaliation - Reuters\n",
      "McDonald’s sales fall worldwide for first time in four years as cost of living bites - The Guardian US\n",
      "Kamala Harris to campaign with VP pick in battleground states next week - CBS News\n",
      "NASA's Perseverance Mars rover finds possible signs of ancient Red Planet life - Space.com\n",
      "SpaceX finds cause of Falcon 9 failure, eyes return to flight as soon as July 27 - Space.com\n",
      "Roy Cooper Is Said to Withdraw From Harris’s Vice-Presidential Field - The New York Times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/dmn0rdm566x7mwf7yj4n74440000gp/T/ipykernel_69396/2917768232.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  articles = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "# Database connection function\n",
    "def get_db_connection():\n",
    "    conn = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='november281996',\n",
    "        database='news_recommendation'\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "# Fetch articles from database\n",
    "def fetch_articles():\n",
    "    conn = get_db_connection()\n",
    "    query = \"SELECT * FROM news_articles\"\n",
    "    articles = pd.read_sql(query, conn)\n",
    "    conn.close()\n",
    "    # Drop duplicate titles\n",
    "    articles = articles.drop_duplicates(subset='title')\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_recommendations(title):\n",
    "    articles = fetch_articles()\n",
    "    \n",
    "    if title not in articles['title'].values:\n",
    "        print(f\"Title '{title}' not found in the dataset.\")\n",
    "        return []  # Title not found\n",
    "    \n",
    "    # Create the TF-IDF matrix\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(articles['title'].fillna(''))\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Re-create the indices to ensure consistency\n",
    "    indices = pd.Series(articles.index, index=articles['title']).drop_duplicates()\n",
    "    idx = indices.get(title, None)\n",
    "    \n",
    "    if idx is None:\n",
    "        print(f\"Title '{title}' not found in the indices.\")\n",
    "        return []  # Title not found\n",
    "\n",
    "    # Ensure idx is within the valid range of cosine_sim\n",
    "    if idx >= cosine_sim.shape[0]:\n",
    "        print(f\"Index {idx} is out of bounds for the cosine similarity matrix.\")\n",
    "        return []\n",
    "    \n",
    "    # Get similarity scores for the given title\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Exclude the original article itself\n",
    "    sim_scores = [(i, score) for i, score in sim_scores if i != idx]\n",
    "    # Sort the articles based on similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get top 10 recommendations\n",
    "    sim_scores = sim_scores[:10]\n",
    "    article_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    return articles['title'].iloc[article_indices].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "title = 'Russian and Chinese bombers intercepted off of Alaska - ABC News'\n",
    "recommendations = get_recommendations(title)\n",
    "for rec in recommendations:\n",
    "    print(rec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
